{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7163287,"sourceType":"datasetVersion","datasetId":4137721}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ipywidgets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Code for classical tokenizer given but used bert tokenizer for fast processing","metadata":{}},{"cell_type":"code","source":"# import re\n# from typing import List\n# import time\n# import string\n# import os\n# import numpy as np\n# import torch\n# from torch import nn\n# from tqdm.notebook import tqdm_notebook as tqdm\n# import nltk\n# from nltk.corpus import stopwords\n# nltk.download('stopwords')\n# nltk.download('punkt')\n\n# start=time.time()\n# MAX_SEQ_LEN=32\n# class Tokenizer_Private():\n#     def __init__(self):\n#         self.data=None\n#     def tokenize(self,data,vocab=None):\n#         self.data=data\n#         if(vocab==None):\n#                 tokenized_text=[nltk.tokenize.word_tokenize(line) for line in self.data]\n#                 flat_token=[token for tokens in tokenized_text for token in tokens]\n#                 self.stop_words=set(stopwords.words('english'))\n#                 filtered_tokens=[token for token in flat_token if token.isalnum() and token not in self.stop_words]\n#                 max_vocab_size=10000\n#                 freq_dist=nltk.probability.FreqDist(filtered_tokens)\n#                 common_words=freq_dist.most_common(max_vocab_size)\n#                 self.vocabulary={word:idx+1 for idx,(word,_) in enumerate(common_words)}\n#         #set self vocab from json\n#         sequence=[]\n#         for text in self.data:\n#             tokens = nltk.tokenize.word_tokenize(text.lower())\n#             filtered_tokens = [token for token in tokens if token.isalnum() and token not in self.stop_words]\n#             numerical_sequence = [self.vocabulary.get(token, 0) for token in filtered_tokens]\n#             sequence.append(numerical_sequence)\n#         return sequence\n\n#     def detokenize(self, inputs):\n#         assert self.data!=None            \n#         tokens=[self.vocabulary.get(idx,\"UNK\") for idx in inputs]\n#         return \" \".join(tokens)\n#     def get_vocabulary(self) -> List[str]:\n#         assert self.data!=None \n#         return list(self.vocabulary.keys())\n#     def token_to_id(self,token):\n#         assert self.data!=None \n#         return self.vocabulary.get(token, 0) \n#     def id_to_token(self,id):\n#         assert self.data!=None \n#         return self.vocabulary.get(id,\"UNK\")\n    \n    \n# class LSTM_Tokenizer:\n#     def __init__(self,data) -> None:\n#         self.data=data \n#         self.tokenizer=Tokenizer_Private()\n#     def tokenize(self):\n#         tokenized=[]\n#         token_list=self.tokenizer.tokenize(self.data)\n#         for data in token_list:\n#             for i in range(len(data)):\n#                 tokens=torch.unsqueeze(torch.tensor(data[:i+1],dtype=torch.int32),dim=0)\n#                 padding_amount = MAX_SEQ_LEN - tokens.size(1)\n#                 padded_sequences = torch.nn.functional.pad(tokens,(padding_amount, 0),mode=\"constant\", value=0)\n#                 tokenized.append(torch.squeeze(padded_sequences,dim=0))\n#         tokenized_tensor=torch.stack(tokenized,dim=0)\n#         return self.tokenizer,tokenized_tensor\n# class Dataset:\n#     def __init__(self,dir):\n#         self.paths=[os.path.join(dir,i) for i in os.listdir(dir) if i.endswith(\"txt\")][:500]\n#     def get_data(self,tokenizer:str=\"LSTM\"):\n#         pages=[]\n#         for i in tqdm(range(len(self.paths))):\n#             with open(self.paths[i],\"r\", encoding=\"iso-8859-1\") as f:\n#                 temp=f.readlines()\n#                 pages.extend(self.preprocess(i) for i in temp if i!=\"\\n\")\n#         assert tokenizer in [\"LSTM\"]\n#         return LSTM_Tokenizer(pages)\n        \n#     def preprocess(self,line):\n#         line=line.lower().strip()\n#         translator = str.maketrans('', '', string.punctuation)\n#         line = line.translate(translator)\n#         line=re.sub(r\"\\n\",\"\",line)\n#         return line\n    \n# dataset=Dataset(\"/kaggle/input/gutenberg/txt\")\n# Token=dataset.get_data(\"LSTM\")\n# tokenizer,x_data=Token.tokenize()\n# print(\"Time to load dataset and obtain tokens is {}\".format(time.time()-start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open(\"vocab.txt\",\"w\") as f:\n#     for i in tokenizer.get_vocabulary():\n#         f.write(i+\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\nfrom torch.utils.data import Dataset, random_split\nimport os\nimport torch\nimport time\nstart=time.time()\nMAX_SEQ_LEN=32\nclass TextDataset(Dataset):\n    def __init__(self, data_dir, max_seq_len=32):\n        self.data_dir = data_dir\n        self.max_seq_len = max_seq_len\n        self.data = []\n        for filename in os.listdir(data_dir)[:500]:\n            if filename.endswith(\".txt\"):\n                with open(os.path.join(data_dir, filename), \"r\",encoding=\"iso-8859-1\") as f:\n                    text = f.read()\n                encoded_inputs = Tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_seq_len)\n                input_ids = encoded_inputs[\"input_ids\"]#attention_mask = encoded_inputs[\"attention_mask\"]\n                for i in range(len(input_ids)):\n                    tokens=torch.unsqueeze(torch.tensor(input_ids[:i+1],dtype=torch.int32),dim=0)\n                    padding_amount = MAX_SEQ_LEN - tokens.size(1)\n                    padded_sequences = torch.nn.functional.pad(tokens,(padding_amount, 0),mode=\"constant\", value=0)\n                    toadd=torch.squeeze(padded_sequences,dim=0)\n                    attention_mask=torch.ones_like(toadd)\n                    self.data.append({\"input_ids\": toadd, \"attention_mask\": attention_mask,})\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n    \ndata_dir = \"/kaggle/input/gutenberg/txt\"\ntokenizer_name = \"bert-base-uncased\"\nTokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\ndataset = TextDataset(data_dir, MAX_SEQ_LEN)\n\n\nprint(\"Time to load using bert fast tokenizer \",time.time()-start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Vocab_Size=len(Tokenizer.get_vocab())\nfrom torch.utils.data import DataLoader\ntrain_loader = DataLoader(dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass LSTMGenerator(nn.Module):\n    def __init__(self, vocab_size, embedding_size=128, hidden_size=1024):\n        super(LSTMGenerator, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.drop = nn.Dropout(0.1)\n        self.forward_lstm = nn.LSTM(embedding_size,512)\n        self.backward_lstm = nn.LSTM(512,320,bidirectional=True)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.linear1 = nn.Linear(640, hidden_size)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, vocab_size)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        embedding = self.embedding(x)\n        drop = self.drop(embedding)\n        forward, _ = self.forward_lstm(drop.permute(1, 0, 2))\n        backward, _ = self.backward_lstm(forward.permute(1, 0, 2))\n        pool = self.pool(backward.permute(0, 2, 1)).squeeze()\n        linear1 = self.relu(self.linear1(pool))\n        linear2 = self.softmax(self.linear2(linear1))\n        return linear2\n        \nclass get_lstm_generator:\n    def  __init__(self,vocab_size:int,embedding_size:int=128,hidden_size:int=1024):\n        self.vocab_size=vocab_size\n        self.embedding_size=embedding_size\n        self.hidden_size=hidden_size\n    def get_model(self):\n        model = LSTMGenerator(self.vocab_size, self.embedding_size, self.hidden_size)\n        loss_fn = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n        return model,loss_fn,optimizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model,loss_fn,optim=get_lstm_generator(Vocab_Size).get_model()\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sanity Checks","metadata":{}},{"cell_type":"code","source":"data=next(iter(train_loader))\ninput_str=data[\"input_ids\"][:,:-1]\noutput_str=data[\"input_ids\"][:,-1]\nout=model(input_str)\nprint(out.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(output_str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_tensor =torch.nn.functional.one_hot(output_str.to(torch.int64), num_classes=Vocab_Size)\nprint(torch.argmax(one_hot_tensor,dim=-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm_notebook\nimport torch\nfrom torch import nn\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\nEPOCHS=100\nmodel,loss_fn,optim=get_lstm_generator(Vocab_Size).get_model()\nassert torch.cuda.device_count()>1\nmodel = torch.nn.DataParallel(model, device_ids= list(range(torch.cuda.device_count())))\nmodel.to(device)\nloss_fn.to(device)\nfor epoch in tqdm_notebook(range(EPOCHS)):\n    epoch_loss=0\n    for i,data in enumerate(train_loader):\n        input_str=data[\"input_ids\"][:,:-1].to(device)\n        output_str=data[\"input_ids\"][:,-1].to(device)\n        predicted=model(input_str)\n        one_hot_tensor =torch.nn.functional.one_hot(output_str.to(torch.int64), num_classes=Vocab_Size).to(torch.float32).to(device)\n        loss=loss_fn(predicted.to(torch.float32),one_hot_tensor)\n        epoch_loss+=loss.detach().cpu().numpy()//8\n        if(i%500==499):\n            print(\"Epoch {} Batch {} Loss {}\".format(epoch,i,loss.detach().cpu().numpy()))\n    print(\"Epoch {} Cumulative Loss :{}\".format(epoch,epoch_loss//2000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TransformerModel(nn.Module):\n    def __init__(self,seq_len, vocab_size, embed_dim, num_heads, num_layers, dropout=0.1):\n        super(TransformerModel, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,batch_first=True),\n            num_layers=num_layers\n        )\n        self.middle=nn.Linear(embed_dim*seq_len,embed_dim)\n        self.transformer_decoder = nn.TransformerDecoder(\n            nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads),\n            num_layers=num_layers\n        )\n\n        self.fc = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, src, tgt):\n        src = self.embedding(src)\n        tgt = self.embedding(tgt)\n\n        memory = self.transformer_encoder(src)\n        memory=self.middle(memory.view(memory.shape[0],-1))\n        output = self.transformer_decoder(tgt,memory)\n\n        output = self.fc(output)\n\n        return output\n\nclass get_transformer_gen:\n    def  __init__(self,vocab_size):\n        self.vocab_size =  vocab_size\n        self.embedding_size = 128\n        self.num_heads = 8\n        self.num_layers = 6  \n    def get_model(self,):\n        model = TransformerModel(31,self.vocab_size, self.embedding_size, self.num_heads,self.num_layers)\n        loss_fn = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n        return model,loss_fn,optimizer   \n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model,loss_fn,optim=get_transformer_gen(Vocab_Size).get_model()\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=next(iter(train_loader))\ninput_str=data[\"input_ids\"][:,:-1]\noutput_str=data[\"input_ids\"][:,-1]\nout=model(input_str,output_str)\nprint(out.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm_notebook\nimport torch\nfrom torch import nn\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\nEPOCHS=100\nmodel,loss_fn,optim=get_transformer_gen(Vocab_Size).get_model()\nassert torch.cuda.device_count()>1\nmodel = torch.nn.DataParallel(model, device_ids= list(range(torch.cuda.device_count())))\nmodel.to(device)\nloss_fn.to(device)\nfor epoch in tqdm_notebook(range(EPOCHS)):\n    epoch_loss=0\n    for i,data in enumerate(train_loader):\n        input_str=data[\"input_ids\"][:,:-1].to(device)\n        output_str=data[\"input_ids\"][:,-1].to(device)\n        predicted=model(input_str,output_str)\n        one_hot_tensor =torch.nn.functional.one_hot(output_str.to(torch.int64), num_classes=Vocab_Size).to(torch.float32).to(device)\n        loss=loss_fn(predicted.to(torch.float32),one_hot_tensor)\n        epoch_loss+=loss.detach().cpu().numpy()//8\n        if(i%500==499):\n            print(\"Epoch {} Batch {} Loss {}\".format(epoch,i,loss.detach().cpu().numpy()))\n    print(\"Epoch {} Cumulative Loss :{}\".format(epoch,epoch_loss))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}