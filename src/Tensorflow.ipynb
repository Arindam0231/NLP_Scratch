{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7163287,"sourceType":"datasetVersion","datasetId":4137721}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tensorflow","metadata":{}},{"cell_type":"markdown","source":"# Processing and Loading","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport re\nfrom typing import List\nimport time\nimport string\nimport os\nimport numpy as np\nfrom keras_nlp.tokenizers import WordPieceTokenizer\nfrom keras_nlp.tokenizers import compute_word_piece_vocabulary\nfrom keras_nlp.tokenizers import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\n\nstart=time.time()\nMAX_SEQ_LEN=32\nclass Tokenizer_Private(Tokenizer):\n    def __init__(self,data) -> None:\n        super().__init__()\n        self.data=data  \n        max_vocab_size=10000\n        tokenized_text=[nltk.tokenize.word_tokenize(line) for line in self.data]\n        flat_token=[token for tokens in tokenized_text for token in tokens]\n        self.stop_words=set(stopwords.words('english'))\n        filtered_tokens=[token for token in flat_token if \n                         token.isalnum() and token not in self.stop_words]\n        freq_dist=nltk.probability.FreqDist(filtered_tokens)\n        common_words=freq_dist.most_common(max_vocab_size)\n        self.vocabulary={word:idx+1 for idx,(word,_) in enumerate(common_words)}\n\n    def tokenize(self,text=None):\n        tokens = nltk.tokenize.word_tokenize(text.lower())\n        filtered_tokens = [token for token in tokens if token.isalnum() and token not in self.stop_words]\n        numerical_sequence = [self.vocabulary.get(token, 0) for token in filtered_tokens]\n        return numerical_sequence\n\n    def detokenize(self, inputs, *args, **kwargs):\n        tokens=[self.vocabulary.get(idx,\"UNK\") for idx in inputs]\n        return \" \".join(tokens)\n    def get_vocabulary(self) -> List[str]:\n        return list(self.vocabulary.keys())\n    def token_to_id(self,token):\n            return self.vocabulary.get(token, 0) \n\n    def id_to_token(self,id): \n        return self.vocabulary.get(id,\"UNK\")\nclass LSTM_Tokenizer:\n    def __init__(self,data) -> None:\n        self.data=data \n        self.tokenizer=Tokenizer_Private(self.data)\n    def tokenize(self):\n        tokenized=[]\n        for line in self.data:\n            token_list=self.tokenizer.tokenize(line)\n            for i in range(len(token_list)):\n                tokenized.append(token_list[:i+1])\n        max_seq_len=max(len(x) for x in tokenized)\n        tokenized=np.array(pad_sequences(tokenized,MAX_SEQ_LEN,padding=\"pre\"))\n        return self.tokenizer,tokenized\nclass Bert_Tokenizer:\n    def  __init__(self,data):\n        self.data=data\n\n    def tokenize(self):\n        vocab_size = 30000\n        dataset = tf.data.Dataset.from_tensor_slices(self.data)\n        vocabulary=compute_word_piece_vocabulary(dataset,\n                                                 vocabulary_size=vocab_size)\n        tokenizer=WordPieceTokenizer(vocabulary=vocabulary,\n                                     sequence_length=MAX_SEQ_LEN,strip_accents=False)            \n            \n        return tokenizer,tokenizer(self.data)\n\nclass Dataset:\n    def __init__(self,dir):\n        self.paths=[os.path.join(dir,i) for i in os.listdir(dir) if i.endswith(\"txt\")]\n    \n    def get_data(self,tokenizer:str=\"BERT\"):\n        pages=[]\n        for i in range(10):\n            with open(self.paths[i],\"r\") as f:\n                temp=f.readlines()\n                pages.extend(self.preprocess(i) for i in temp if i!=\"\\n\")\n        assert tokenizer in [\"BERT\",\"LSTM\"]\n        if tokenizer==\"BERT\":\n            return Bert_Tokenizer(pages)\n        else:\n            return LSTM_Tokenizer(pages)\n        \n    def preprocess(self,line):\n        line=line.lower().strip()\n        translator = str.maketrans('', '', string.punctuation)\n        line = line.translate(translator)\n        line=re.sub(r\"\\n\",\"\",line)\n        return line\n    \ndataset=Dataset(\"/kaggle/input/gutenberg/txt\")\ntokenizer,x_data=dataset.get_data(\"LSTM\").tokenize()\nprint(\"Time to load dataset and obtain tokens is {}\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:30:53.896328Z","iopub.execute_input":"2023-12-10T08:30:53.896910Z","iopub.status.idle":"2023-12-10T08:31:17.302530Z","shell.execute_reply.started":"2023-12-10T08:30:53.896878Z","shell.execute_reply":"2023-12-10T08:31:17.301532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary=tokenizer.get_vocabulary()\nprint(len(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:31:17.303942Z","iopub.execute_input":"2023-12-10T08:31:17.304336Z","iopub.status.idle":"2023-12-10T08:31:17.310879Z","shell.execute_reply.started":"2023-12-10T08:31:17.304298Z","shell.execute_reply":"2023-12-10T08:31:17.309852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(x_data))\nprint(x_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:31:17.312924Z","iopub.execute_input":"2023-12-10T08:31:17.313246Z","iopub.status.idle":"2023-12-10T08:31:17.328292Z","shell.execute_reply.started":"2023-12-10T08:31:17.313217Z","shell.execute_reply":"2023-12-10T08:31:17.327296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Architecture","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras.layers as nn\n\nclass LSTM_Generator(tf.keras.Model):\n    def __init__(self,vocab_size:int,embedding_size:int=128,hidden_size:int=1024):\n        super(LSTM_Generator,self).__init__()\n        self.embedding=nn.Embedding(vocab_size,embedding_size)\n        self.drop=nn.Dropout(0.1)\n        self.forward=nn.LSTM(512,return_sequences=True)\n        self.backward=nn.Bidirectional(nn.LSTM(320,return_sequences=True))\n        self.pool=nn.GlobalMaxPooling1D()\n        self.linear1=nn.Dense(hidden_size,activation=\"relu\")\n        self.linear2=nn.Dense(vocab_size,activation=\"softmax\")\n        \n    def call(self,x:tf.Tensor)->tf.Tensor:\n        embedding=self.embedding(x)\n        drop=self.drop(embedding)\n        forward=self.forward(drop)\n        backward=self.backward(forward)\n        pool=self.pool(backward)\n        linear1=self.linear1(pool)\n        linear2=self.linear2(linear1)\n        return linear2\n        \nclass get_lstm_generator:\n    def  __init__(self,vocab_size:int,embedding_size:int=128,hidden_size:int=1024):\n        self.vocab_size=vocab_size\n        self.embedding_size=embedding_size\n        self.hidden_size=hidden_size\n    def getModel(self)->tf.keras.Model:\n\n        loss_fn=tf.keras.losses.CategoricalCrossentropy()\n        optim=tf.keras.optimizers.Adam(learning_rate=0.1)\n        model=LSTM_Generator(self.vocab_size,self.embedding_size,self.hidden_size)\n        model.compile(optimizer=optim,loss=loss_fn,metrics=[\"accuracy\"])\n        return model","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:31:17.329389Z","iopub.execute_input":"2023-12-10T08:31:17.329708Z","iopub.status.idle":"2023-12-10T08:31:17.343101Z","shell.execute_reply.started":"2023-12-10T08:31:17.329670Z","shell.execute_reply":"2023-12-10T08:31:17.342263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ndef preprocess_lstm(tokenized_inputs,vocab_size):\n    x_train=tokenized_inputs[:,:-1]\n    labels=tokenized_inputs[:,-1]\n    y_train=to_categorical(labels,num_classes=vocab_size+1)\n    train=tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(batch_size=8,num_parallel_calls=tf.data.AUTOTUNE)\n    return train\ntrain=preprocess_lstm(x_data,len(vocabulary))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:31:17.344136Z","iopub.execute_input":"2023-12-10T08:31:17.344426Z","iopub.status.idle":"2023-12-10T08:31:48.087005Z","shell.execute_reply.started":"2023-12-10T08:31:17.344401Z","shell.execute_reply":"2023-12-10T08:31:48.086005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=get_lstm_generator(len(vocabulary)+1).getModel()\nmodel.fit(train,epochs=100)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:31:48.088338Z","iopub.execute_input":"2023-12-10T08:31:48.089213Z","iopub.status.idle":"2023-12-10T08:35:35.577040Z","shell.execute_reply.started":"2023-12-10T08:31:48.089168Z","shell.execute_reply":"2023-12-10T08:35:35.575310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Architecture","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras.layers as nn\nimport keras_nlp\n\n\nSEQ_LENGTH = 128\nMASK_RATE = 0.25\nPREDICTIONS_PER_SEQ = 32\n\n# Model params.\nNUM_LAYERS = 3\nMODEL_DIM = 256\nINTERMEDIATE_DIM = 512\nNUM_HEADS = 4\nDROPOUT = 0.1\nNORM_EPSILON = 1e-5\nclass Transformer_Learner(tf.keras.Model):\n    def __init__(self,batch_size:int,num_head:int,vocab_size:int,seq_length:int=128,embedding_size:int=128,hidden_size:int=1024):\n        super(Transformer_Learner,self).__init__()\n        self.embedding_layer=keras_nlp.layers.TokenAndPositionEmbedding(vocab_size,seq_length,embedding_size)\n        self.encoder_model=tf.keras.Sequential([nn.Input(batch_shape=(batch_size, seq_length)),\n                                                self.embedding_layer,\n                                          nn.LayerNormalization(),\n                                          nn.Dropout(0.1)])\n        for _ in range(num_head):\n            self.encoder_model.add(keras_nlp.layers.TransformerEncoder(hidden_size,num_head,dropout=0.1,layer_norm_epsilon=1e-4))\n        \n        self.outputs = keras_nlp.layers.MaskedLMHead(token_embedding=self.embedding_layer.token_embedding,activation=\"softmax\",)\n    def call(self,x:tf.Tensor)->tf.Tensor:\n        token_id,mask_pos=x\n        output_token=self.encoder_model(token_id)\n        output=self.outputs(output_token, mask_positions=mask_pos)\n        return output\n\nclass get_transformer_learner:\n    def  __init__(self,batch_size,num_head:int,vocab_size:int,embedding_size:int=128,hidden_size:int=1024):\n        self.batch_size=batch_size\n        self.vocab_size=vocab_size\n        self.num_head=num_head\n        self.embedding_size=embedding_size\n        self.hidden_size=hidden_size\n    def getModel(self)->tf.keras.Model:\n        loss_fn=tf.keras.losses.SparseCategoricalCrossentropy()\n        optim=tf.keras.optimizers.Adam(learning_rate=1e-2)\n        model=Transformer_Learner(self.batch_size,self.num_head,self.vocab_size,self.embedding_size,self.hidden_size)\n        model.compile(optimizer=optim,loss=loss_fn,metrics=[\"accuracy\"])\n        return model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport keras.layers as nn\nimport keras_nlp\n\n\nSEQ_LENGTH = 128\nMASK_RATE = 0.25\nPREDICTIONS_PER_SEQ = 32\n\nNUM_LAYERS = 3\nMODEL_DIM = 256\nINTERMEDIATE_DIM = 512\nNUM_HEADS = 4\nDROPOUT = 0.1\nNORM_EPSILON = 1e-5\n\nclass Transformer_Decoder(tf.keras.Model):\n    def __init__(self,batch_size:int,num_head:int,vocab_size:int,seq_length:int=128,embedding_size:int=128,hidden_size:int=1024):\n        super(Transformer_Decoder,self).__init__()\n        self.encoder=get_transformer_learner.getModel()\n        self.embedding_layer=keras_nlp.layers.TokenAndPositionEmbedding(vocab_size,seq_length,embedding_size)\n#         Check Shape today\n        self.decoder_model=tf.keras.Sequential([nn.Input(batch_shape=(batch_size, seq_length)),\n                                                self.embedding_layer,\n                                          nn.Dropout(0.1)])\n        for _ in range(num_head):\n            self.decoder_model.add(keras_nlp.layers.TransformerDecoder(hidden_size,num_head,dropout=0.1,layer_norm_epsilon=1e-4))\n        \n       \n    def call(self,x:tf.Tensor)->tf.Tensor:\n        token_id,mask_pos=x\n        output_token=self.deccoder_model(token_id)\n        return output_token\n\nclass get_transformer_decoder:\n    def  __init__(self,batch_size,num_head:int,vocab_size:int,embedding_size:int=128,hidden_size:int=1024):\n        self.batch_size=batch_size\n        self.vocab_size=vocab_size\n        self.num_head=num_head\n        self.embedding_size=embedding_size\n        self.hidden_size=hidden_size\n    def getModel(self)->tf.keras.Model:\n        loss_fn=tf.keras.losses.SparseCategoricalCrossentropy()\n        optim=tf.keras.optimizers.Adam(learning_rate=1e-2)\n        model=Transformer_Decoder(self.batch_size,self.num_head,self.vocab_size,self.embedding_size,self.hidden_size)\n        model.compile(optimizer=optim,loss=loss_fn,metrics=[\"accuracy\"])\n        return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Masking For Transformer","metadata":{}},{"cell_type":"code","source":"masker=keras_nlp.layers.MaskedLMMaskGenerator(\nvocabulary_size=len(vocabulary),\nmask_selection_rate=0.25,\nmask_selection_length=32,\nmask_token_id=tokenizer.token_to_id(\"[MASK]\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_transformer(tokenized_inputs):\n    outputs = masker(tokenized_inputs)\n    features = {\n        \"token_ids\": outputs[\"token_ids\"],\n        \"mask_positions\": tf.cast(outputs[\"mask_positions\"],tf.int32),\n    }\n    labels = outputs[\"mask_ids\"]\n    weights = outputs[\"mask_weights\"]\n    return features, labels, weights\n\nfeatures,labels,weights=preprocess_transformer(x_data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Masking","metadata":{}},{"cell_type":"code","source":"print(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features[\"token_ids\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model= get_transformer_learner(64,3,len(vocabulary)).getModel()\nmodel.fit([features[\"token_ids\"],features[\"mask_positions\"]],labels,batch_size=64,epochs=100,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output=model.predict([features[\"token_ids\"][:5],features[\"mask_positions\"][:5]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(output.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.argmax(output))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(labels[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.detokenize(tf.argmax(output,axis=-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}